# Gradient: partial derivative of a function in respect to all of its independent variables 
# Gradient Descent: optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient
# Backpropagation: a method used to calculate the gradient of the loss function with respect to the weights of the network
# Loss Function: a method of evaluating how well the algorithm models the dataset
# Activation Function: a function that determines the output of a neural network
# Epoch: one forward pass and one backward pass of all the training examples
# Batch Size: the number of training examples utilized in one iteration
# Learning Rate: a hyperparameter that controls how much we are adjusting the weights of our network with respect to the loss gradient
# Momentum: a method that helps accelerate gradients vectors in the right directions, thus leading to faster converging
# Regularization: a technique used to reduce overfitting by discouraging overly complex models in some way
# Dropout: a technique used to prevent overfitting by randomly setting some neurons to zero during forward and backward pass
# Overfitting: a model that is too complex and fits the training data too well
# Underfitting: a model that is too simple and does not fit the training data well
# Hyperparameter: a parameter whose value is set before the learning process begins
# Standardize: transforming data to have mean=0 and standard deviation=1 to ensure all features contribute equally